---
title: Retrieval-Augmented Generation (RAG)
date: 2024-08-12T16:00:00.000Z
keywords: 'ai, rag, llm, artificial inteligence, machine learning, openai'
---

\### Retrieval-Augmented Generation (RAG)

RAG has two critical elements: Vector DB and Embedding model.

Let's see why they are needed.

1\. The LLM input context is limited, so RAG is needed. We cannot use all of our documents as LLM input; thus, we must select the most relevant parts.

2\. We need to consider semantic similarity when selecting relevant parts of the documents. For example, when the user asks for a "list of fruits," document parts containing a list of fruits (e.g., orange, grape, strawberry) might be relevant. The best we can do without semantic similarity is lexical similarity (which elastic search does). The embedding model does the job of translating the text into a vector. To get a rough idea about the vectorization process, look for "word2vec".

3\. Finally, we need to save our vector somewhere, so we need a vector database. Some vector DBs, like chroma, can also help us calculate the "distance" between two vectors. The most common way to calculate the distance between vectors is by using "cosine similarity" (i.e., it calculates the angle instead of the Euclidean distance).

Here is a little demonstration: I have a document containing a list of foods (including vegetables, fruits, etc.), and I specifically ask for a "list of fruits." The VectorDB successfully returns "Banana, Orange, Grape, Melon, Pasta, Cake, Ramen" as the closest part of the document.

\### RAG Techniques

!\[Techniques]\(https\://res.cloudinary.com/dqo6txtrv/image/upload/v1723511235/Natserract%20Blog/prw02yeqo88tpvzvmham.jpg)

\### Additional Reading

\- https\://medium.com/madhukarkumar/how-to-build-enterprise-ai-apps-with-multi-agent-rag-systems-mars-f922f69f59ba
