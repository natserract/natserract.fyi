---
title: Retrieval-Augmented Generation (RAG)
date: 2024-08-12T16:00:00.000Z
keywords: 'ai, rag, llm, artificial inteligence, machine learning, openai'
---

## Retrieval-Augmented Generation (RAG)

RAG has two critical elements: Vector DB and Embedding model.

Let's see why they are needed.

1\. The LLM input context is limited, so RAG is needed. We cannot use all of our documents as LLM input; thus, we must select the most relevant parts.

2\. We need to consider semantic similarity when selecting relevant parts of the documents. For example, when the user asks for a "list of fruits," document parts containing a list of fruits (e.g., orange, grape, strawberry) might be relevant. The best we can do without semantic similarity is lexical similarity (which elastic search does). The embedding model does the job of translating the text into a vector. To get a rough idea about the vectorization process, look for "word2vec".

3\. Finally, we need to save our vector somewhere, so we need a vector database. Some vector DBs, like chroma, can also help us calculate the "distance" between two vectors. The most common way to calculate the distance between vectors is by using "cosine similarity" (i.e., it calculates the angle instead of the Euclidean distance).

Here is a little demonstration: I have a document containing a list of foods (including vegetables, fruits, etc.), and I specifically ask for a "list of fruits." The VectorDB successfully returns "Banana, Orange, Grape, Melon, Pasta, Cake, Ramen" as the closest part of the document.

## RAG Techniques

![](https://res.cloudinary.com/dqo6txtrv/image/upload/v1723511235/Natserract%20Blog/prw02yeqo88tpvzvmham.jpg)

## RAG Multiple Strategies

Stop relying on just cosine similarity for building search and RAG systems. Check out all the popular retrieval
strategies illustrated and more in my FREE short course on Improving RAG
Systems! One of the most critical components in a RAG system is the
retrieval strategy.Â Do not ignore it and use only default settings.

Strategies mentioned below and also in the course:

1. Similarity-based retrieval: Simple embedding similarity, you can play
   around with different metrics like cosine, maximal margin relevance etc.
2. Similarity with Threshold retrieval: Caps the similarity threshold and
   can help in removing irrelevant document but can drop recall
3. Multi-query retrieval: Make LLMs generate variations of your query and
   get documents based on these multi-versions of your initial query, often
   useful to get more recall or coverage of documents
4. Reranker retrieval: Uses cross encoders specialized in ranking documents based on search relevance
5. Hybrid Retrieval: Uses keyword search and semantic search similarity,
   gets combined documents, and reranks with fusion reranker

Do explore these techiques and share with others if helpful!

![](https://res.cloudinary.com/dqo6txtrv/image/upload/v1728118180/Natserract%20Blog/1728055825805_i2vuzq.gif)

Source: [https://www.linkedin.com/posts/dipanjans\_stop-relying-on-just-cosine-similarity-for-activity-7247991470358478848-kvcw?utm\_source=share\&utm\_medium=member\_desktop](https://www.linkedin.com/posts/dipanjans_stop-relying-on-just-cosine-similarity-for-activity-7247991470358478848-kvcw?utm_source=share\&utm_medium=member_desktop)

## Additional Reading

* [https://medium.com/madhukarkumar/how-to-build-enterprise-ai-apps-with-multi-agent-rag-systems-mars-f922f69f59ba](https://medium.com/madhukarkumar/how-to-build-enterprise-ai-apps-with-multi-agent-rag-systems-mars-f922f69f59ba)
* [https://www.answer.ai/](https://www.answer.ai/)
